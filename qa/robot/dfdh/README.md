## Digital Forms systems testing

Build up test to increase confidence in deployment over time. 

Things that could be tested with PyTest:

- Test form submission validation. Things like this, but cases may be handled by 
    - Missing mandatory fields?
    - Invalid values, field lengths, null, format/data type
- Verify presentation types:
    - ADP can only be ORAL, WRIT, CANCELLED, NONE
    - IRP can only be ORAL, WRIT, CANCELLED, NONE
    - URL can only be WRIT, CANCELLED, NONE
- Verify review role types:
    - APPNT, LWYR, AUTHPERS
- Verify logging is working
- Verify disclosure document is attached to message attachment correctly.
- Verify e-mail notifications sent as expected.
- Verify token and sessions work for PayBC API pod
- Verify any reports or activity details generated by the system?
- Verify pod is running expected build number.
- Failure scenarios
    - Pods frozen or shut down? Are alerts sent?
    - PayBC?
    - DFAPI? e.g. scheduling failure?
    - Splunk?
    - Invalid request(s) from PayBC?
- Other scenarios in PyTest:
    - See below

## Digital Forms smoketest

Things that could be tested post-deployment:

BEFORE SMOKETEST, after deployment
    - Record build number, git branch, commit hash, day and time of deployment
    - Capture and archive deployment logs 
    - Capture and archive deployment duration 
    - Enable debug output in TEST environment, in case of problems
    - Build report or compare build output to previous build(s)?
    - Ensure pods are running expected code

1. Submit form applications:
    - ADP (00)
    - IRP (21, 40)
    - UL (30)
    - Confirm notifications sent (check console logs?)
    - Confirm submissions:
        - Get application details from DFAPI to confirm saved correctly
        - Check Rabbit queue has pending forms?
3. Update application field (is this possible from DFDH?)
    - Query application fields
    - Make field change
    - Query application to confirm changes made as expected
4. Make a payment
    - Check payment status
    - Attempt to schedule a review, which should fail (?)
    - Submit payment
    - Check payment status again to confirm
    - Attempt to submit another application:
5. Schedule a review
    - Confirm message sent to applicant
    - Attempt to book outside of allowed window (past, future), should fail (?)
    - Schedule a review
    - Schedule a second review, should fail (?)
6. Mark disclosure as sent
    - Mark non-existant disclosure as sent, should cause error (?)
7. External step: log on to VIPS and mark review as completed. **MANUAL STEP. HOW TO AUTOMATE?**
    - One set of ADP, IRP, UL successful on first try.
    - Second set of ADP, IRP, UL unsuccessful at first 
    - Submit second applications, for ADP, IRP, UL:
        - ADP and IRP second submissions rejected (already applied)
        - UL second submission accepted
8. UL nth application testing:
    - Update second application
    - Second payment
    - Second review schedule
    - Second disclosure sent
    - Third application before result? Should fail because second application still in progress.
    - EXTERNAL STEP: mark review as completed. **MANUAL STEP. HOW TO AUTOMATE?**
    - Forth application, after review marked as complete.


AFTER SMOKETEST
- Capture and archive save pod logs from 
    - Note time range that can be used for a future Splunk query, if Splunk stores logs for a long time.
    - Search and report on logs for potential problems (exceptions, errors, warnings, strings like "unexepected", "fail", "time out"...)?
    - Some kind of log diff to see if significant changes between builds?

